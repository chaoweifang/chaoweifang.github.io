[{"authors":["admin"],"categories":null,"content":"Chaowei FANG is an associate professor at School of Artificial Intelligence, Xidian University, and also a member of the Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education. He received Ph.D. degree from The University of Hong Kong in 2019, and B.E. degree from Xi'an Jiaotong University in 2013. His research interest includes visual information processing, representation, and understanding. His email is: chaoweifang AT outlook.com.\r","date":1747267200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1747267200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://chaoweifang.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Chaowei FANG is an associate professor at School of Artificial Intelligence, Xidian University, and also a member of the Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education. He received Ph.D. degree from The University of Hong Kong in 2019, and B.E. degree from Xi'an Jiaotong University in 2013. His research interest includes visual information processing, representation, and understanding. His email is: chaoweifang AT outlook.com.\r","tags":null,"title":"Chaowei Fang","type":"authors"},{"authors":["Chaowei Fang","Bolin Fu","De Cheng","Lechao Cheng","Guanbin Li"],"categories":null,"content":"","date":1753660800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1753660800,"objectID":"71658a6fba27dfecf945b857d5abb7a2","permalink":"https://chaoweifang.github.io/publication/tmm25realistic-image-sr/","publishdate":"2025-07-28T00:00:00Z","relpermalink":"/publication/tmm25realistic-image-sr/","section":"publication","summary":"Realistic image super-resolution (SR) focuses on transforming real-world low-resolution (LR) images into high-resolution (HR) ones, handling more complex degradation patterns than synthetic SR tasks. This is critical for applications like surveillance, medical imaging, and consumer electronics. However, current methods struggle with limited real-world LR-HR data, impacting the learning of basic image features. Pre-trained SR models from large-scale synthetic datasets offer valuable prior knowledge, which can improve generalization, speed up training, and reduce the need for extensive real-world data in realistic SR tasks. In this paper, we introduce a novel approach, Dual-domain Adaptation Networks, which is able to efficiently adapt pre-trained image SR models from simulated to real-world datasets. To achieve this target, we first set up a spatial-domain adaptation strategy through selectively updating parameters of pre-trained models and employing the low-rank adaptation technique to adjust frozen parameters. Recognizing that image super-resolution involves recovering high-frequency components, we further integrate a frequency domain adaptation branch into the adapted model, which combines the spectral data of the input and the spatial-domain backbone's intermediate features to infer HR frequency maps, enhancing the SR result. Experimental evaluations on public realistic image SR benchmarks, including RealSR, D2CRealSR, and DRealSR, demonstrate the superiority of our proposed method over existing state-of-the-art models. Codes are available at: https://github.com/dummerchen/DAN.","tags":["Image Super-resolution","Low-level Image Processing"],"title":"Dual-domain Adaptation Networks for Realistic Image Super-resolution","type":"publication"},{"authors":["Chaowei Fang","Bolin Fu","De Cheng","Lechao Cheng","Dingwen Zhang"],"categories":null,"content":"","date":1752019200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1752019200,"objectID":"afef99fda16b42206b8edead9d3a4a40","permalink":"https://chaoweifang.github.io/publication/pr25high-frequency-structure/","publishdate":"2025-07-09T00:00:00Z","relpermalink":"/publication/pr25high-frequency-structure/","section":"publication","summary":"Magnetic Resonance (MR) imaging is essential in clinical diagnostics due to its ability to capture detailed soft tissue structures. However, acquiring high-resolution MR images is expensive and often leads to reduced signal-to-noise ratios. To address this, MR image super-resolution aims to generate high-resolution images from low-resolution inputs. While deep neural networks have been widely applied for MR image superresolution, they struggle to effectively utilize structural information critical for accurate reconstruction. This paper introduces a novel Transformer-based framework for super-resolving T2-weighted MR image which is a critical MR imaging modality. This framework excels in leveraging both intra-modality and inter-modality dependencies to enhance the structural information. The innovative component of our proposed architecture is termed as High-frequency Structure Transformer (HFST) which operates on the gradients of input images, leveraging the high-frequency structure prior. It also employs high-resolution T1-weighted images which is a more efficient MR imaging modality to provide substantial inter-modality structure priors for the processing of low-resolution T2-weighted images. HFST is featured by parallel intra-modality and inter-modality context exploration and window-based self-attention modules. Notably, both intra-head and inter-head correlations are incorporated to build up the self-attention modules, amplifying the relation extraction capacity. Rigorous evaluations on three benchmarks including IXI, BraTS2018, and fastMRI reveal that our method sets a new state of the art in MR image super-resolution. Especially, our method increases the PSNR metric by up to 1.28 dB under the 4× super-resolution setting. Our codes are available at https://github.com/dummerchen/HFST.","tags":["Low-level Image Processing","Medical Image Analysis","Image Super-resolution"],"title":"High-frequency structure transformer for magnetic resonance image super-resolution","type":"publication"},{"authors":["Zihao Mo","Junye Chen","Chaowei Fang","Guanbin Li"],"categories":null,"content":"","date":1752019200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1752019200,"objectID":"b2fde6250b03916680470418b5ab81ae","permalink":"https://chaoweifang.github.io/publication/acmmm2025patchwiper/","publishdate":"2025-07-09T00:00:00Z","relpermalink":"/publication/acmmm2025patchwiper/","section":"publication","summary":"Visible watermark removal is crucial for evaluating watermark robustness and advancing more resilient protection techniques. Current methods face challenges in real-world scenarios due to  architectural constraints in multi-task frameworks and limited dataset diversity. To address these challenges, we first propose a novel two-stage framework, PatchWiper, consisting of an independent watermark segmentation network and a highly dynamic patch-wise restoration network. This framework decouples watermark localization from background restoration, allowing each network to focus on its designated task. Our restoration network dynamically generates unique parameters for each image patch, enabling fine-grained adaptation to different watermark distortions. Second, we construct the Pixabay Real-world Watermark Dataset (PRWD), which incorporates diverse background images and over 1,000 distinct watermark types, providing a more comprehensive benchmark for evaluating watermark removal methods. Extensive experiments on PRWD, ILAW, and real-world testing images demonstrate our method's superior performance over existing approaches, particularly in handling complex real-world cases.","tags":["low-level image processing","visible watermark removal"],"title":"PatchWiper: Leveraging Dynamic Patch-Wise Parameters for Real-World Visible Watermark Removal","type":"publication"},{"authors":["Fangwen Wu","Lechao Cheng","Shengeng Tang","Xiaofeng Zhu","Chaowei Fang","Dingwen Zhang","Meng Wang"],"categories":null,"content":"","date":1747872000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1747872000,"objectID":"fb57a6de620760641a02c35e222418da","permalink":"https://chaoweifang.github.io/publication/icml25navigating/","publishdate":"2025-05-22T00:00:00Z","relpermalink":"/publication/icml25navigating/","section":"publication","summary":"Class-incremental learning (CIL) seeks to enable a model to sequentially learn new classes while retaining knowledge of previously learned ones. Balancing flexibility and stability remains a significant challenge, particularly when the task ID is unknown. To address this, our study reveals that the gap in feature distribution between novel and existing tasks is primarily driven by differences in mean and covariance moments. Building on this insight, we propose a novel semantic drift calibration method that incorporates mean shift compensation and covariance calibration. Specifically, we calculate each class's mean by averaging its sample embeddings and estimate task shifts using weighted embedding changes based on their proximity to the previous mean, effectively capturing mean shifts for all learned classes with each new task. We also apply Mahalanobis distance constraint for covariance calibration, aligning class-specific embedding covariances between old and current networks to mitigate the covariance shift. Additionally, we integrate a feature-level self-distillation approach to enhance generalization. Comprehensive experiments on commonly used datasets demonstrate the effectiveness of our approach. The source code is available at https://github.com/fwu11/MACIL.git.","tags":["Image Classification","Class-Incremental Learning"],"title":"Navigating Semantic Drift in Task-Agnostic Class-Incremental Learning","type":"publication"},{"authors":["Junye Chen","Chaowei Fang","Jichang Li","Yicheng Leng","Guanbin Li"],"categories":null,"content":"","date":1747267200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1747267200,"objectID":"a95fbb800549dcaf13073c90f7ff6523","permalink":"https://chaoweifang.github.io/publication/tip25decouple/","publishdate":"2025-05-15T00:00:00Z","relpermalink":"/publication/tip25decouple/","section":"publication","summary":"This paper aims to restore original background images in watermarked videos, overcoming challenges posed by traditional approaches that fail to handle the temporal dynamics and diverse watermark characteristics effectively. Our method introduces a unique framework that first “decouples” the extraction of prior knowledge—such as common-sense knowledge and residual background details—from the temporal modeling process, allowing for independent handling of background restoration and temporal consistency. Subsequently, it “couples” these extracted features by integrating them into the temporal modeling backbone of a video inpainting (VI) framework. This integration is facilitated by a specialized module, which includes an intrinsic background image prediction sub-module and a dual-branch frame embedding module, designed to reduce watermark interference and enhance the application of prior knowledge. Moreover, a frame-adaptive feature selection module dynamically adjusts the extraction of prior features based on the corruption level of each frame, ensuring their effective incorporation into the temporal processing. Extensive experiments on YouTube-VOS and DAVIS datasets validate our method’s efficiency in watermark removal and background restoration, showing significant improvement over state-of-the-art techniques in visible image watermark removal, video restoration, and video inpainting.","tags":["Visible Watermark Removal","Low-level Image Processing"],"title":"Decouple and Couple: Exploiting Prior Knowledge for Visible Video Watermark Removal","type":"publication"},{"authors":["De Cheng","Lei Wei","Chaowei Fang","Lingfeng He","Nannan Wang","Xinbo Gao"],"categories":null,"content":"","date":1747267200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1747267200,"objectID":"f72eae2a3a9db632d8077fd20acf6567","permalink":"https://chaoweifang.github.io/publication/tcsvt25progressive/","publishdate":"2025-05-15T00:00:00Z","relpermalink":"/publication/tcsvt25progressive/","section":"publication","summary":"Transductive zero-shot learning (TZSL) has been proposed to address the domain shift problem by leveraging additional unlabeled unseen data to enhance the generalization ability from seen classes to unseen target classes. Existing TZSL methods primarily focus on mitigating the distribution bias problem by incorporating these unlabeled samples into the generative models. Although these methods have achieved great success, they do not fully exploit the potential of these unlabeled target data. In this paper, we propose a bidirectional weakly guided conditional generative modeling approach, which utilizes the attribute regressor and the visual generator to synthesize paired training data of unseen classes for each other, thus converting unlabeled target data into matched feature-attribute pairs. Additionally, on top of the generative modeling, we also propose to progressively estimate the associations between visual features and attributes among the unlabeled target data through a semi-supervised pseudo-labeling approach, so as to further facilitate the generative model and enhance the learning of target distributions. Extensive experimental results on four benchmark datasets demonstrate the effectiveness of the proposed method, achieving superior performances to state-of-the-art methods. Our source code is released in: https://github.com/LevisWei/semi-zero-master.","tags":["Zero-shot Learning","Image Classification"],"title":"Progressive Feature-Attribute Matching via Bi-directional Generation for Transductive Zero-Shot Learning","type":"publication"},{"authors":["Chengqing Cai","Shuangyi Tan","Xinmiao Wang","Bohao Zhang","Chaowei Fang","Guanbin Li","Longqin Xu","Shuangyin Liu","Ruixin Wang"],"categories":null,"content":"","date":1747267200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1747267200,"objectID":"d5874fa0721522177aeba763e227ac82","permalink":"https://chaoweifang.github.io/publication/aquaculture25real-time/","publishdate":"2025-05-15T00:00:00Z","relpermalink":"/publication/aquaculture25real-time/","section":"publication","summary":"Prolonged hypoxic conditions pose a significant threat to the survival of fish in aquaculture, often leading to mass mortality events. Abnormal fish behavior, particularly under hypoxic stress, can serve as an early warning indicator of decreasing dissolved oxygen levels in water. However, existing methods for detecting hypoxia stress behavior in fish are affected by the lighting, occlusion, and turbidity in real aquaculture environments. This results in low accuracy in detecting hypoxia stress behaviors. In this paper, we propose a real-time detection method, YOLOv8n-HSB, designed to enhance the accuracy of detecting hypoxia stress behavior in tilapia within recirculating aquaculture systems. Key improvements of our approach include: (1) the introduction of the Multi-scale Fusion Pyramid Network (MFP-Net), which enhances small object detection by adding a specific layer at the bottom of the feature pyramid and improving feature fusion based on Bi-directional Feature Pyramid Network (BIfpn) architecture for the neck structure; (2) the development of the C2f-Occlusion Perception (C2f-OP) module in the backbone by integrating Mobile Inverted Residual Bottleneck Convolution (MBConv) and Effective Squeeze-and-Excitation (ESE), improving the model’s ability to capture crucial local features; and (3) the replacement of conventional Convolution (Conv) layers with Dynamic Convolution (DConv) modules integrated with ParameterNet (P-DConv), enhancing the model’s capacity to process complex information and extract fine-scale features of fish. Experimental results demonstrate that the YOLOv8n-HSB model offers a highly effective solution for detecting hypoxia stress behavior in tilapia. Compared to the original YOLOv8n model, the AP@0.5:0.95 increases by 4.05%. The AP@0.5 reaches 96.12%, outperforming existing state-of-the-art methods. This study provides a novel method for monitoring the abnormal behavior of fish in hypoxic environments, offering practical significance for smart aquaculture systems.","tags":["Object Detection"],"title":"Real-Time Detection of Hypoxia Stress Behavior in Aquaculture Fish Using an Enhanced YOLOv8 Model","type":"publication"},{"authors":["Xiaoqian Zhu","Xiangrong Zhang","Tianyang Zhang","Chaowei Fang","Xu Tang","Licheng Jiao"],"categories":null,"content":"","date":1747267200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1747267200,"objectID":"aecca9b492a8421baafaa614a471c37b","permalink":"https://chaoweifang.github.io/publication/ijcai25regionmatch/","publishdate":"2025-05-15T00:00:00Z","relpermalink":"/publication/ijcai25regionmatch/","section":"publication","summary":"Semi-supervised semantic segmentation (S4) has shown significant promise in reducing the burden of labor-intensive data annotation. However, existing methods mainly rely on pixel-level information, neglecting the strong region consistency inherent in remote sensing images (RSIs), which limits their effectiveness in handling the complex and diverse backgrounds of RSIs. To address this, we propose RegionMatch, a novel approach that leverages unlabeled data from a fresh object-level perspective, which is more tailored to the nature of semantic segmentation. We design the Pixel-Region Synergy Pseudo-Labeling strategy, which explicitly injects object-level contextual information into the S4 pipeline and promotes knowledge collaboration between pixel and region perspectives for generating high-quality pseudo-labels. In addition, we propose the Region Structure-Aware Correlation Consistency, which models object-level relationships by establishing inter-region correlations across images and pixel correlations within regions, providing more effective supervision signals for unlabeled data. Experimental results demonstrate that RegionMatch outperforms state-of-the-art methods on multiple authoritative remote sensing datasets, highlighting its superiority in the RSIs.","tags":["Label-efficient Machine Learning","Semi-Supervised Learning","Image Segmentation","Remote Sensing"],"title":"RegionMatch: Pixel-Region Collaboration for Semi-Supervised Semantic Segmentation in Remote Sensing Images","type":"publication"},{"authors":["Chaowei Fang","Hangfei Ma","Zhihao Li","De Cheng","Yue Zhang","Guanbin Li"],"categories":null,"content":"","date":1747267200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1747267200,"objectID":"c8dea9449d6a5695ebaf84d45ea094bb","permalink":"https://chaoweifang.github.io/publication/ijcai25screening/","publishdate":"2025-05-15T00:00:00Z","relpermalink":"/publication/ijcai25screening/","section":"publication","summary":"Pre-trained vision-language models have shown remarkable potential for downstream tasks. However, their fine-tuning under noisy labels remains an open problem due to challenges like self-confirmation bias and the limitations of conventional small-loss criteria. In this paper, we propose a unified framework to address these issues, consisting of three key steps':' Screening, Rectifying, and Re-Screening. First, a dual-level semantic matching mechanism is introduced to categorize samples into clean, ambiguous, and noisy samples by leveraging both macro-level and micro-level textual prompts. Second, we design tailored pseudo-labeling strategies to rectify noisy and ambiguous labels, enabling their effective incorporation into the training process. Finally, a re-screening step, utilizing cross-validation with an auxiliary vision-language model, mitigates self-confirmation bias and enhances the robustness of the framework. Extensive experiments across ten datasets demonstrate that the proposed method significantly outperforms existing approaches for tuning vision-language pre-trained models with noisy labels.","tags":["Label-efficient Machine Learning","Learning with Noisy Labels","Prompt Tuning","Visual Language Models"],"title":"Screening, Rectifying, and Re-Screening: A Unified Framework for Tuning Vision-Language Models with Noisy Labels","type":"publication"},{"authors":["Yue Zhang","Yiyi Chen","Chaowei Fang","Qian Wang","Jiayi Wu","Jingmin Xin"],"categories":null,"content":"","date":1734998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1734998400,"objectID":"e8ee6f92dd21ee2ec1068bfcdb7d7743","permalink":"https://chaoweifang.github.io/publication/pr25learning-from-openset-noisy-labels/","publishdate":"2024-12-24T00:00:00Z","relpermalink":"/publication/pr25learning-from-openset-noisy-labels/","section":"publication","summary":"In this paper, we propose a novel method to address the challenge of learning deep neural network models in the presence of open-set noisy labels, which include mislabeled samples from out-of-distribution categories. Previous methods relied on the distances between sample-wise predictions and labels to identify mislabeled samples and distinguish between in-distribution (ID) and out-of-distribution (OOD) noisy samples, which struggle to promptly identify the two types of noisy samples. To overcome these limitations, we propose a novel method that utilizes feature information and cross-instance relationships, enabling a more comprehensive distinction between ID and OOD noisy samples. Our approach involves a multi-prototype modeling mechanism, where each class is represented by multiple prototypes to account for the diversity within categories. This mechanism helps in distinguishing in-distribution and out-of-distribution noisy samples by comparing sample features with class prototypes. We introduce an online algorithm for updating prototypes and enhancing model optimization with cross-augmentation consistency and a noise-robust contrastive siamese learning technique. Our extensive experiments on datasets like CIFAR100, Clothing1M, and Food101N show our method’s superiority in handling noisy labels compared to existing approaches.","tags":["Label-efficient Machine Learning","Image Classification"],"title":"Learning from Open-set Noisy Labels based on Multi-prototype Modeling","type":"publication"},{"authors":["Yicheng Leng","Chaowei Fang","Junye Chen","Yixiang Fang","Sheng Li","Guanbin Li"],"categories":null,"content":"","date":1733788800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1733788800,"objectID":"0e9915b6fb685004e1b0fec4c53a72ba","permalink":"https://chaoweifang.github.io/publication/aaai2025bridging_knowledge_gap/","publishdate":"2024-12-10T00:00:00Z","relpermalink":"/publication/aaai2025bridging_knowledge_gap/","section":"publication","summary":"Visible watermark removal which involves watermark cleaning and background content restoration is pivotal to evaluate the resilience of watermarks. Existing deep neural network (DNN)-based models still struggle with large-area watermarks and are overly dependent on the quality of watermark mask prediction. To overcome these challenges, we introduce a novel feature adapting framework that leverages the representation modeling capacity of a pre-trained image inpainting model. Our approach bridges the knowledge gap between image inpainting and watermark removal by fusing information of the residual background content beneath watermarks into the inpainting backbone model. We establish a dual-branch system to capture and embed features from the residual background content, which are merged into intermediate features of the inpainting backbone model via gated feature fusion modules. Moreover, for relieving the dependence on high-quality watermark masks, we introduce a new training paradigm by utilizing coarse watermark masks to guide the inference process. This contributes to a visible image removal model which is insensitive to the quality of watermark mask during testing. Extensive experiments on both a large-scale synthesized dataset and a real-world dataset demonstrate that our approach significantly outperforms existing state-of-the-art methods. The source code is available in the supplementary materials.","tags":["Low-level Image Processing","Visible Watermark Removal"],"title":"Bridging Knowledge Gap between Image Inpainting and Large-Area Visible Watermark Removal","type":"publication"},{"authors":["Zhenhua Wu","Linxuan Jiang","Xiang Li","Chaowei Fang","Yipeng Qin","Guanbin Li"],"categories":null,"content":"","date":1733788800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1733788800,"objectID":"2f8d3e7986863e41b4dbf3581a8c08c6","permalink":"https://chaoweifang.github.io/publication/aaai2025hierarchically_controlled_deformable/","publishdate":"2024-12-10T00:00:00Z","relpermalink":"/publication/aaai2025hierarchically_controlled_deformable/","section":"publication","summary":"Audio-driven talking head synthesis is a critical task in digital human modeling. While recent advances using diffusion models and Neural Radiance Fields (NeRF) have improved visual quality, they often require substantial computational resources, limiting practical deployment. We present a novel framework for audio-driven talking head synthesis, namely it Hierarchically Controlled Deformable 3D Gaussians (HiCoDe), which achieves state-of-the-art performance with significantly reduced computational costs. Our key contribution is a hierarchical control strategy that effectively bridges the gap between sparse audio features and dense 3D Gaussian point clouds. Specifically, this strategy comprises two control levels: i) coarse-level control based on a 3D Morphable Model (3DMM) and ii) fine-level control using facial landmarks. Extensive experiments on the HDTF dataset and additional test sets demonstrate that our method outperforms existing approaches in visual quality, facial landmark accuracy, and audio-visual synchronization while being more computationally efficient in both training and inference.","tags":["Talking Head Generation","Gaussian Splatting"],"title":"Hierarchically Controlled Deformable 3D Gaussians for Talking Head Synthesis","type":"publication"},{"authors":["Sen Chen","Hongying Liu","Chaowei Fang","Fanhua Shang","Yuanyuan Liu","Liang Wan","Dongmei Jiang","Yaowei Wang"],"categories":null,"content":"","date":1733788800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1733788800,"objectID":"036dc12c3d06e634d031be8b7a88c8ae","permalink":"https://chaoweifang.github.io/publication/aaai2025unsupervised_degradation_representation/","publishdate":"2024-12-10T00:00:00Z","relpermalink":"/publication/aaai2025unsupervised_degradation_representation/","section":"publication","summary":"Blind image super-resolution (blind SR) aims to restore a high-resolution (HR) image from a low-resolution (LR) image with unknown degradation. Many existing methods explicitly estimate degradation information from various LR images. However, in most cases, image degradations are independent of image content. Their estimations may be influenced by the image content resulting in inaccuracy. Unlike existing works, we design a dual-encoder for degradation representation (DEDR) to preclude the influence of image content from LR images. This benefits in extracting the intrinsic degradation representation more accurately. To the best of our knowledge, this paper is the first work that estimates degradation representation through filtering out image content. Based on the degradation representation extracted by DEDR, we present a novel framework, named degradation representation aware transform network (DRAT) for blind SR. We propose global degradation aware (GDA) blocks to propagate degradation information across spatial and channel dimensions, in which a degradation representation transform module (DRT) is introduced to render features degradation-aware, thereby enhancing the restoration of LR images. Extensive experiments are conducted on three benchmark datasets (including Gaussian 8, DIV2KRK, and real-world datasets) under large scaling factors with complex degradations. The experimental results demonstrate that DRAT surpasses state-of-the-art supervised kernel estimation and unsupervised degradation prediction methods. The code will be released on GitHub.","tags":["Low-level Image Processing","Image Super-resolution"],"title":"Unsupervised Degradation Representation Aware Transform for Real-World Blind Image Super-Resolution","type":"publication"},{"authors":["Ying Yang","De Cheng","Chaowei Fang","Yubiao Wang","Changzhe Jiao","Lechao Cheng","Nannan Wang","Xinbo Gao"],"categories":null,"content":"","date":1729728000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1729728000,"objectID":"65c2ddb5716ed97dfea169011801ab93","permalink":"https://chaoweifang.github.io/publication/nips24ood-detection/","publishdate":"2024-10-24T00:00:00Z","relpermalink":"/publication/nips24ood-detection/","section":"publication","summary":"Unsupervised out-of-distribution (OOD) detection aims to identify out-of-domain data by learning only from unlabeled In-Distribution (ID) training samples, which is crucial for developing a safe real-world machine learning system. Current reconstruction-based method provides a good alternative approach, by measuring the reconstruction error between the input and its corresponding generative counterpart in the pixel/feature space. However, such generative methods face the key dilemma, i.e., improving the reconstruction power of the generative model, while keeping compact representation of the ID data. To address this issue, we propose the diffusion-based layer-wise semantic reconstruction approach for unsupervised OOD detection. The innovation of our approach is that we leverage the diffusion model’s intrinsic data reconstruction ability to distinguish ID samples from OOD samples in the latent feature space. Moreover, to set up a comprehensive and discriminative feature representation, we devise a multi-layer semantic feature extraction strategy. Through distorting the extracted features with Gaussian noises and applying the diffusion model for feature reconstruction, the separation of ID and OOD samples is implemented according to the reconstruction errors. Extensive experimental results on multiple benchmarks built upon various datasets demonstrate that our method achieves state-of-the-art performance in terms of detection accuracy and speed. Code is available at https://github.com/xbyym/DLSR.","tags":["OOD Detection","Diffusion Model"],"title":"Diffusion-based Layer-wise Semantic Reconstruction for Unsupervised Out-of-Distribution Detection","type":"publication"},{"authors":["De Cheng","Haichun Tai","Nannan Wang","Chaowei Fang","Xinbo Gao"],"categories":null,"content":"","date":1725840000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1725840000,"objectID":"07a4066a002103f911cc851b58225f82","permalink":"https://chaoweifang.github.io/publication/tifs24neighbor-consistency/","publishdate":"2024-09-09T00:00:00Z","relpermalink":"/publication/tifs24neighbor-consistency/","section":"publication","summary":"Unsupervised person re-identification (ReID) aims at learning discriminative identity features for person retrieval without any annotations. Recent advances accomplish this task by leveraging clustering-based pseudo labels, but these pseudo labels are inevitably noisy, which deteriorates model performance. In this paper, we propose a Neighbour Consistency guided Pseudo Label Refinement (NCPLR) framework, which can be regarded as a transductive form of label propagation under the assumption that the prediction of each example should be similar to its nearest neighbours’. Specifically, the refined label for each training instance can be obtained from the original clustering result and a weighted ensemble of its neighbours’ predictions, with weights determined according to their similarities in the feature space. Furthermore, we also explore building a unified global-local NCPLR mechanism through a global-local label interaction module to achieve mutual label refinement. Such a strategy promotes efficient complementary learning while mitigating some unreliable information, finally improving the quality of the refined pseudo labels for each global-local region. Extensive experimental results demonstrate the effectiveness of the proposed method, showing superior performance to state-of-the-art methods by a large margin. Our source code is released in https://github.com/haichuntai/NCPLR-ReID.","tags":["Person Re-Identification","Unsupervised Learnig"],"title":"Neighbor Consistency and Global-Local Interaction: A Novel Pseudo-label Refinement Approach for Unsupervised Person Re-Identification","type":"publication"},{"authors":["Yubo Li","De Cheng","Chaowei Fang","Changzhe Jiao","Nannan Wang","Xinbo Gao"],"categories":null,"content":"","date":1724025600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1724025600,"objectID":"3ce8b42a760788272173b986c8012abc","permalink":"https://chaoweifang.github.io/publication/acmmm2024ccreid/","publishdate":"2024-08-19T00:00:00Z","relpermalink":"/publication/acmmm2024ccreid/","section":"publication","summary":"Cloth-Changing Person Re-Identification (CC-ReID) aims to accurately identify a target person in the more realistic surveillance scenario where clothes of the pedestrian may change drastically, which is critical in public security systems for tracking down disguised criminal suspects. Existing methods mainly transform the CC-ReID problem into cross-modality feature alignment from the data-driven perspective, without modelling the interference factors such as clothes and camera view changes meticulously. This may lead to over-consideration or under-consideration of the influence of these factors on the extraction of robust and discriminative identity features. This paper proposes a novel algorithm for thoroughly disentangling identity features from interference factors brought by clothes and camera view changes while ensuring the robustness and discriminability. It adopts a dual-stream identity feature learning framework consisting of a raw image stream and a cloth-erasing stream, to explore discriminative and cloth-irrelevant identity feature representations. Specifically, an adaptive cloth-irrelevant contrastive objective is introduced to contrast features extracted by the two streams, aiming to suppress the fluctuation caused by clothes textures in the identity feature space. Moreover, we innovatively mitigate the influence of the interference factors through a generative adversarial interference factor decoupling network. This network is targeted at capturing identity-related information residing in the interference factors and disentangling the identity features from such information. Extensive experimental results demonstrate the effectiveness of the proposed method, achieving superior performances to state-of-the-art methods. ","tags":["person re-identification"],"title":"Disentangling Identity Features from Interference Factors for Cloth-Changing Person Re-Identification","type":"publication"},{"authors":["Guanbin Li","Zhuohua Chen","Mingzhi Mao","Liang Lin","Chaowei Fang"],"categories":null,"content":"","date":1717718400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1717718400,"objectID":"da093c20ffbe45218e079478abdaee9c","permalink":"https://chaoweifang.github.io/publication/tip24uncertainty-aware/","publishdate":"2024-06-07T00:00:00Z","relpermalink":"/publication/tip24uncertainty-aware/","section":"publication","summary":"Due to the advancement of deep learning, the performance of salient object detection (SOD) has been significantly improved. However, deep learning-based techniques require a sizable amount of pixel-wise annotations. To relieve the burden of data annotation, a variety of deep weakly-supervised and unsupervised SOD methods have been proposed, yet the performance gap between them and fully supervised methods remains significant. In this paper, we propose a novel, cost-efficient salient object detection framework, which can adapt models from synthetic data to real-world data with the help of a limited number of actively selected annotations. Specifically, we first construct a synthetic SOD dataset by copying and pasting foreground objects into pure background images. With the masks of foreground objects taken as the ground-truth saliency maps, this dataset can be used for training the SOD model initially. However, due to the large domain gap between synthetic images and real-world images, the performance of the initially trained model on the real-world images is deficient. To transfer the model from the synthetic dataset to the real-world datasets, we further design an uncertainty-aware active domain adaptive algorithm to generate labels for the real-world target images. The prediction variances against data augmentations are utilized to calculate the superpixel-level uncertainty values. For those superpixels with relatively low uncertainty, we directly generate pseudo labels according to the network predictions. Meanwhile, we select a few superpixels with high uncertainty scores and assign labels to them manually. This labeling strategy is capable of generating high-quality labels without incurring too much annotation cost. Experimental results on six benchmark SOD datasets demonstrate that our method outperforms the existing state-of-the-art weakly-supervised and unsupervised SOD methods and is even comparable to the fully supervised ones.","tags":["Salient Object Detection","Domain Adaptation","Active Learning"],"title":"Uncertainty-aware Active Domain Adaptive Salient Object Detection","type":"publication"},{"authors":["Yuzhu Wang","Lechao Cheng","Chaowei Fang","Dingwen Zhang","Manni Duan","Meng Wang"],"categories":null,"content":"","date":1716681600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1716681600,"objectID":"42d243dc957debbff2858004934ba8fc","permalink":"https://chaoweifang.github.io/publication/icml24revisitingpowerofprompt/","publishdate":"2024-05-26T00:00:00Z","relpermalink":"/publication/icml24revisitingpowerofprompt/","section":"publication","summary":"","tags":["Image Classification","Prompt Tuning"],"title":"Revisiting the Power of Prompt for Visual Tuning","type":"publication"},{"authors":["Kaihui Yang","Junwei Han","Guangyu Guo","Chaowei Fang","Yingzi Fan","Lechao Cheng","Dingwen Zhang"],"categories":null,"content":"","date":1713830400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1713830400,"objectID":"4936c770172b3de63a63518e0abe44ff","permalink":"https://chaoweifang.github.io/publication/tomm24progressive-adapting-and-pruning/","publishdate":"2024-04-23T00:00:00Z","relpermalink":"/publication/tomm24progressive-adapting-and-pruning/","section":"publication","summary":"Saliency prediction (SAP) plays a crucial role in simulating the visual perception function of human beings. In practical situations, humans can quickly grasp saliency extraction in new image domains. However, current SAP methods mainly concentrate on training models in single domains, which do not effectively handle diverse content and styles present in real-world images. As a result, it would be of great significance if SAP models could efficiently adjust to new image domains. To this end, this paper aims to design SAP models that can imitate the incremental learning ability of human beings on multiple image domains, and name domain-incremental saliency prediction (DISAP). To make a trade-off between preventing the forgetting of historical domains and achieving high performance on new domains, we propose a progressively updated domain incremental encoder. This encoder consists of a domain-sharing branch and a domain-specific branch. The domain-sharing branch includes a feature selection mechanism to preserve crucial parameters after fine-tuning the model on each current domain. The remaining parameters are reserved to absorb knowledge from future domains. Furthermore, to capture the unique characteristics of each domain with relatively low computational overhead, we introduce a lightweight design to construct the domain-specific branch, enabling effective adaptation to new domains. Extensive experiments are conducted on multiple domain-incremental learning settings formed by four saliency prediction datasets, including Salicon, MIT1003, the art subset of CAT2000, and WebSal. The results demonstrate that our method outperforms existing methods significantly. The code is available at  https://github.com/KaIi-github/DIL4SAP.","tags":["Saliency Prediction","Continual Learning"],"title":"Progressive Adapting and Pruning: Domain-Incremental Learning for Saliency Prediction","type":"publication"},{"authors":["Deguang Chen","Jianrui Chen","Chaowei Fang","Zhichao Zhang"],"categories":null,"content":"","date":1712016000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1712016000,"objectID":"fae8a2c17336c078c95a985cca077bd0","permalink":"https://chaoweifang.github.io/publication/appint24cvqa/","publishdate":"2024-04-02T00:00:00Z","relpermalink":"/publication/appint24cvqa/","section":"publication","summary":"","tags":["Visual Question Answering"],"title":"Complex visual question answering based on uniform form and content","type":"publication"},{"authors":["Jingxuan He","Lechao Cheng","Chaowei Fang","Zunlei Feng","Tingting Mu","Mingli Song"],"categories":null,"content":"","date":1703203200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1703203200,"objectID":"a4cbca2e6c12d0b334d5d90bbcbedf0f","permalink":"https://chaoweifang.github.io/publication/aaai2024progressive/","publishdate":"2023-12-22T00:00:00Z","relpermalink":"/publication/aaai2024progressive/","section":"publication","summary":"","tags":["Image Segmentation","Weakly Supervised Learning","Label-efficient Machine Learning"],"title":"Progressive Feature Self-Reinforcement for Weakly Supervised Semantic Segmentation","type":"publication"},{"authors":["Yicheng Leng","Chaowei Fang","Gen Li","Yixiang Fang","Guanbin Li"],"categories":null,"content":"","date":1703203200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1703203200,"objectID":"6d8f5531d68f4b3e77844baea9ef3485","permalink":"https://chaoweifang.github.io/publication/aaai2024removing/","publishdate":"2023-12-22T00:00:00Z","relpermalink":"/publication/aaai2024removing/","section":"publication","summary":"","tags":["Low-level Image Processing","Visible Watermark Removal"],"title":"Removing Interference and Recovering Content Imaginatively for Visible Watermark Removal","type":"publication"},{"authors":["Chaowei Fang","Ziyin Zhou","Junye Chen","Hanjing Su","Qingyao Wu","Guanbin Li"],"categories":null,"content":"","date":1703203200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1703203200,"objectID":"494517f5b8960c18b5cfba6bf9b3a86d","permalink":"https://chaoweifang.github.io/publication/aaai2024interactiveseg/","publishdate":"2023-12-22T00:00:00Z","relpermalink":"/publication/aaai2024interactiveseg/","section":"publication","summary":"","tags":["Image Segmentation","Human-Computer Interaction"],"title":"Variance-insensitive and Target-preserving Mask Refinement for Interactive Image Segmentation","type":"publication"},{"authors":["Dingwen Zhang","Hao Li","Wenyuan Zeng","Chaowei Fang","Lechao Cheng","Ming-Ming Cheng","and Junwei Han"],"categories":null,"content":"","date":1703203200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1703203200,"objectID":"76f6450bf88d388e6abc7303cde82908","permalink":"https://chaoweifang.github.io/publication/tip23weakly/","publishdate":"2023-12-22T00:00:00Z","relpermalink":"/publication/tip23weakly/","section":"publication","summary":"","tags":["Image Segmentation","Weakly Supervised Learning","Label-efficient Machine Learning"],"title":"Weakly Supervised Semantic Segmentation via Alternate Self-Dual Teaching","type":"publication"},{"authors":["Ziyi Zhang","Weikai Chen","Chaowei Fang","Zhen Li","Lechao Cheng","Liang Lin","Guanbin Li"],"categories":null,"content":"","date":1695427200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1695427200,"objectID":"f394cc122eafa344bce408737d6d4f87","permalink":"https://chaoweifang.github.io/publication/iccv2023rankmatch/","publishdate":"2023-09-23T00:00:00Z","relpermalink":"/publication/iccv2023rankmatch/","section":"publication","summary":"","tags":["Label-efficient Machine Learning","Image Classification"],"title":"RankMatch: Fostering Confidence and Consistency in Learning with Noisy Labels","type":"publication"},{"authors":["Ruifei Zhang","Feng Zhang","Dejun Fan","Chaowei Fang","Xiang Wan","Guanbin Li","Xutao Lin"],"categories":null,"content":"","date":1688083200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1688083200,"objectID":"fd6a57a9ca5869c3d0fadd256b01ddf6","permalink":"https://chaoweifang.github.io/publication/jbhi2023mlhg/","publishdate":"2023-06-30T00:00:00Z","relpermalink":"/publication/jbhi2023mlhg/","section":"publication","summary":"","tags":["Medical Image Analysis"],"title":"Multi-task Learning with Hierarchical Guidance for Locating and Stratifying Submucosal Tumors","type":"publication"},{"authors":["Shuang Wang","Qi Zang","Chaowei Fang","Dou Quan","Yutong Wan","Biao Hou","Licheng Jiao"],"categories":null,"content":"","date":1687996800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1687996800,"objectID":"21187e27e35aaa3e8f2e7b3e81f89107","permalink":"https://chaoweifang.github.io/publication/tnnls23spe/","publishdate":"2023-06-29T00:00:00Z","relpermalink":"/publication/tnnls23spe/","section":"publication","summary":"Accurately extracting buildings from aerial images has essential research significance for timely understanding human intervention on the land. The distribution discrepancies between diversified unlabeled remote sensing images (changes in imaging sensor, location, and environment) and labeled historical images significantly degrade the generalization performance of deep learning algorithms. Unsupervised domain adaptation (UDA) algorithms have recently been proposed to eliminate the distribution discrepancies without re-annotating training data for new domains. Nevertheless, due to the limited information provided by a single source domain, single-source UDA is not an optimal choice when multi-temporal and multi-region remote sensing images are available. We propose a multi-source UDA (MSUDA) framework SPENet for building extraction, aiming at selecting, purifying and exchanging information from multisource domains to better adapt the model to the target domain. Specifically, the framework effectively utilizes richer knowledge by extracting target-relevant information from multiple source domains, purifying target domain information with low-level features of buildings, and exchanging target domain information in a collaborative learning manner. Extensive experiments and ablation studies constructed on twelve city datasets prove the effectiveness of our method against existing state-of-the-art methods, e.g., our method achieves 59.1% IoU on Austin and Kitsap - Potsdam, which surpasses the target domain supervised method by 2.2%.","tags":["Image Segmentation","Unsupervised Domain Adaptation","Remote Sensing"],"title":"Select, Purify and Exchange: A Multi-Source Unsupervised Domain Adaptation Method for Building Extraction","type":"publication"},{"authors":["Chaowei Fang","Lechao Cheng","Yining Mao","Dingwen Zhang","Yixiang Fang","Guanbin Li","Huiyan Qi","Licheng Jiao"],"categories":null,"content":"","date":1687996800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1687996800,"objectID":"cc9d3be076c9f0aba410198d3ccd5dfa","permalink":"https://chaoweifang.github.io/publication/tnnls23lln-ltd/","publishdate":"2023-06-29T00:00:00Z","relpermalink":"/publication/tnnls23lln-ltd/","section":"publication","summary":"Most existing methods that cope with noisy labels usually assume that the class-wise data distributions are well balanced. They are difficult to deal with the practical scenarios where training samples have imbalanced distributions, since they are not able to differentiate noisy samples from tail classes' clean samples. This paper makes an early effort to tackle the image classification task in which the provided labels are noisy and have a long-tailed distribution. To deal with this problem, we propose a new learning paradigm which can screen out noisy samples by matching between inferences on weak and strong data augmentations. A leave-noise-out regularization is further introduced to eliminate the effect of the recognized noisy samples. Besides, we propose a prediction penalty based on the online class-wise confidence levels to avoid the bias towards easy classes which are dominated by head classes. Extensive experiments on five datasets including CIFAR-10, CIFAR-100, MNIST, FashionMNIST, and Clothing1M demonstrate that the proposed method outperforms existing algorithms for learning with long-tailed distribution and label noise.","tags":["Imbalanced Learning","Noisy Label Learning","Image Classification"],"title":"Separating Noisy Samples from Tail Classes for Long-Tailed Image Classification with Label Noise","type":"publication"},{"authors":["Weizhi Zhong","Chaowei Fang","Yinqi Cai","Pengxu Wei","Gangming Zhao","Liang Lin","Guanbin Li"],"categories":null,"content":"","date":1678752000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1678752000,"objectID":"ed20f85582c44e30c0ff8659f4426ca1","permalink":"https://chaoweifang.github.io/publication/cvpr2023identity/","publishdate":"2023-03-14T00:00:00Z","relpermalink":"/publication/cvpr2023identity/","section":"publication","summary":"","tags":["Image Synthesis","Talking Face Generation"],"title":"Identity-Preserving Talking Face Generation with Landmark and Appearance Priors","type":"publication"},{"authors":["Chaowei Fang","Qian Wang","Lechao Cheng","Zhifan Gao","Chengwei Pan","Zhen Cao","Zhaohui Zheng","Dingwen Zhang"],"categories":null,"content":"","date":1678752000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1678752000,"objectID":"6613c68d9c3801b02ea17d1acdef7918","permalink":"https://chaoweifang.github.io/publication/tmi23reliable/","publishdate":"2023-03-14T00:00:00Z","relpermalink":"/publication/tmi23reliable/","section":"publication","summary":"Convolutional neural networks (CNNs) have made enormous progress in medical image segmentation. The learning of CNNs is dependent on a large amount of training data with fine annotations. The workload of data labeling can be significantly relieved via collecting imperfect annotations which only match the underlying ground truths coarsely. However, label noises which are systematically introduced by the annotation protocols, severely hinders the learning of CNN-based segmentation models. Hence, we devise a novel collaborative learning framework in which two segmentation models cooperate to combat label noises in coarse annotations. First, the complementary knowledge of two models is explored by making one model clean training data for the other model. Secondly, to further alleviate the negative impact of label noises and make sufficient usage of the training data, the specific reliable knowledge of each model is distilled into the other model with augmentation-based consistency constraints. A reliability-aware sample selection strategy is incorporated for guaranteeing the quality of the distilled knowledge. Moreover, we employ joint data and model augmentations to expand the usage of reliable knowledge. Extensive experiments on two benchmarks showcase the superiority of our proposed method against existing methods under annotations with different noise levels. For example, our approach can improve existing methods by nearly 3% DSC on the lung lesion segmentation dataset LIDC-IDRI under annotations with 80% noise ratio. Code is available at: https://github.com/Amber-Believe/ReliableMutualDistillation.","tags":["Medical Image Analysis","Image Segmentation","Label-efficient Machine Learning"],"title":"Reliable Mutual Distillation for Medical Image Segmentation under Imperfect Annotations","type":"publication"},{"authors":["Yuxiang Nie","Chaowei Fang","Lechao Cheng","Liang Lin","Guanbin Li"],"categories":null,"content":"","date":1668816000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668816000,"objectID":"a6ae417dc7f86aa2dcb2e6aad7251931","permalink":"https://chaoweifang.github.io/publication/aaai2023adapt/","publishdate":"2022-11-19T00:00:00Z","relpermalink":"/publication/aaai2023adapt/","section":"publication","summary":"Recently, semi-supervised learning attracts extensive interests in the field of object detection, since it is beneficial for alleviating the label annotation burden. Existing methods generates unsatisfactory pseudo labels and have imperfect performance on the detection of small objects. In this paper, we propose a novel semi-supervised learning algorithm for object detection, based on cross-scale siamese representation learning. This helps improve the generalization of learned features, thus reducing the dependence to high-quality annotations. Furthermore, we devise box refinement and adaptive thresholding strategies to generate higher-quality pseudo labels for unlabeled images. Experiments on VOC and COCO demonstrate that our method achieves new state of the art. Specially, it brings gain of 1 mAP on COCO with 10% labeled images.","tags":["Object Detection","Semi-supervised Learning"],"title":"Adapting Object Size Variance and Class Imbalance for Semi-Supervised Object Detection","type":"publication"},{"authors":["Kuo Wang","Jingyu Zhuang","Guanbin Li","Chaowei Fang","Lechao Cheng","Liang Lin","Fan Zhou"],"categories":null,"content":"","date":1668816000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668816000,"objectID":"ce972e25eae72c9bb2da119d10a3f568","permalink":"https://chaoweifang.github.io/publication/aaai2023debiased/","publishdate":"2022-11-19T00:00:00Z","relpermalink":"/publication/aaai2023debiased/","section":"publication","summary":"Most of the recent research in semi-supervised object detection follows the pseudo-labeling paradigm evolved from the semi-supervised image classification task. However, the training paradigm of the two-stage object detector inevitably makes the pseudo-label learning process for unlabeled images full of bias. Specifically, the IoU matching scheme used for selecting and labeling candidate boxes is based on the assumption that the matching source~(ground truth) is accurate enough in terms of the number of objects, object position and object category. Obviously, pseudo-labels generated for unlabeled images cannot satisfy such a strong assumption, which makes the produced training proposals extremely unreliable and thus severely spoil the follow-up training. To de-bias the training proposals generated by the pseudo-label-based IoU matching, we propose a general framework -- De-biased Teacher, which abandons both the IoU matching and pseudo labeling processes by directly generating favorable training proposals for consistency regularization between the weak/strong augmented image pairs. Moreover, a distribution-based refinement scheme is designed to eliminate the scattered class predictions of significantly low values for higher efficiency. Extensive experiments demonstrate that the proposed De-biased Teacher consistently outperforms other state-of-the-art methods on the MS-COCO and PASCAL VOC benchmarks. Source code would be made available.","tags":["Object Detection","Semi-supervised Learning"],"title":"De-biased Teacher: Rethinking IoU Matching for Semi-Supervised Object Detection","type":"publication"},{"authors":["Chengwei Pan","Baolian Qi","Gangming Zhao","Jiaheng Liu","Chaowei Fang","Dingwen Zhang","Jinpeng Li"],"categories":null,"content":"","date":1668816000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668816000,"objectID":"0b153ee78056f477456a29e6807eecb7","permalink":"https://chaoweifang.github.io/publication/bibm2022deep3dvessel/","publishdate":"2022-11-19T00:00:00Z","relpermalink":"/publication/bibm2022deep3dvessel/","section":"publication","summary":"","tags":["3D Vessel Segmentation","Transformer","Medical Image Processing"],"title":"Deep 3D Vessel Segmentation based on Cross Transformer Network","type":"publication"},{"authors":null,"categories":null,"content":"","date":1668816000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668816000,"objectID":"1264c3bca143cd7b57b27b75e6d2a8c2","permalink":"https://chaoweifang.github.io/post/mtap2022specialissue/","publishdate":"2022-11-19T00:00:00Z","relpermalink":"/post/mtap2022specialissue/","section":"post","summary":"","tags":null,"title":"Welcome for submitting your manuscript to the special issue Human-centric Multimedia Analysis at Multimedia Tools and Applications.(Submission deadline has passed.)","type":"post"},{"authors":["Lechao Cheng","Chaowei Fang","Dingwen Zhang","Guanbin Li","Gang Huang"],"categories":null,"content":"","date":1659052800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1659052800,"objectID":"b168e327a423064a049a472bdeefbe52","permalink":"https://chaoweifang.github.io/publication/acmmm2022cbn-lt/","publishdate":"2022-07-29T00:00:00Z","relpermalink":"/publication/acmmm2022cbn-lt/","section":"publication","summary":"","tags":["Imbalanced Learning","Image Classification"],"title":"Compound Batch Normalization for Long-tailed Image Classification","type":"publication"},{"authors":["Chengwei Pan","Gangming Zhao","Junjie Fang","Jiaheng Liu","Chaowei Fang","Dingwen Zhang","Jinpeng Li","Yizhou Yu"],"categories":null,"content":"","date":1659052800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1659052800,"objectID":"27d64160b6ac6dd2e98d834b8834f1a8","permalink":"https://chaoweifang.github.io/publication/miccai22ars/","publishdate":"2022-07-29T00:00:00Z","relpermalink":"/publication/miccai22ars/","section":"publication","summary":"","tags":["Medical Image Analysis","Object Detection","Disease Diagnosis","Chest X-rays"],"title":"Computer-aided Tuberculosis Diagnosis with Attribute Reasoning Assistance","type":"publication"},{"authors":["Chaowei Fang","Dingwen Zhang","Liang Wang","Yulun Zhang","Lechao Cheng","Junwei Han"],"categories":null,"content":"","date":1659052800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1659052800,"objectID":"ce60c2eca23b175072cc6866c622a727","permalink":"https://chaoweifang.github.io/publication/acmmm2022cohf-t/","publishdate":"2022-07-29T00:00:00Z","relpermalink":"/publication/acmmm2022cohf-t/","section":"publication","summary":"","tags":["Neural Network Design","Image Super-Resolution","Medical Image Analysis","Low-level Image Processing"],"title":"Cross-Modality High-Frequency Transformer for MR Image Super-Resolution","type":"publication"},{"authors":["Jiaming Li","Chaowei Fang","Guanbin Li"],"categories":null,"content":"","date":1659052800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1659052800,"objectID":"aeb6051de3d7c35bc9f2e46a03dc6233","permalink":"https://chaoweifang.github.io/publication/prcv22grum/","publishdate":"2022-07-29T00:00:00Z","relpermalink":"/publication/prcv22grum/","section":"publication","summary":"","tags":["Image Segmentation","Label-efficient Machine Learning","Unsupervised Domain Adaptation","Medical Image Analysis"],"title":"Gradient-Rebalanced Uncertainty Minimization for Cross-Site Adaptation of Medical Image Segmentation","type":"publication"},{"authors":["Kuo Wang","Yuxiang Nie","Chaowei Fang","Chengzhi Han","Xuewen Wu","Xiaohui Wang","Liang Lin","Fan Zhou","Guanbin Li"],"categories":null,"content":"","date":1652400000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1652400000,"objectID":"d77631f6e649184a11e7753da5b41dc7","permalink":"https://chaoweifang.github.io/publication/ijcai2022dcst/","publishdate":"2022-05-13T00:00:00Z","relpermalink":"/publication/ijcai2022dcst/","section":"publication","summary":"In the semi-supervised object detection task, due to the scarcity of labeled data and the diversity and complexity of objects to be detected, the quality of pseudo-labels generated by existing methods for unlabeled data is relatively low, which severely restricts the performance of semi-supervised object detection. In this paper, we revisit the pseudo-labeling based Teacher-Student mutual learning framework for semi-supervised object detection and identify that the inconsistency of the location and feature of the candidate object proposals between the Teacher and the Student branches are the fatal cause of the low quality of the pseudo labels. To address this issue, we propose a simple yet effective technique within the mainstream teacher-student framework, called Double Check Soft Teacher, to overcome the harm caused by insufficient quality of pseudo labels. Specifically, our proposed method leverages teacher model to generate pseudo labels for the student model. Especially, the candidate boxes generated by the student model based on the pseudo label will be sent to the teacher model for \"double check\", and then the teacher model will output probabilistic soft label with background class for those candidate boxes, which will be used to train the student model. Together with a pseudo labeling mechanism based on the sum of the TOP-K prediction score, which improves the recall rate of pseudo labels, Double Check Soft Teacher consistently surpasses state-of-the-art methods by significant margins on the MS-COCO benchmark, pushing the new state-of-the-art. Source code would be made available.","tags":["Label-efficient Machine Learning","Object Detection","Semi-supervised Learning"],"title":"Double-Check Soft Teacher for Semi-Supervised Object Detection","type":"publication"},{"authors":["Chaowei Fang","Liang Wang","Dingwen Zhang","Jun Xu","Yixuan Yuan","Junwei Han"],"categories":null,"content":"","date":1652400000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1652400000,"objectID":"d6fe7fa6f7c271ff718ac39e4c80cae3","permalink":"https://chaoweifang.github.io/publication/cvpr2022slicesyn/","publishdate":"2022-05-13T00:00:00Z","relpermalink":"/publication/cvpr2022slicesyn/","section":"publication","summary":"","tags":["Medical Image Analysis","Low-level Image Processing","Image Interpolation"],"title":"Incremental Cross-view Mutual Distillation for Self-supervised Medical CT Synthesis","type":"publication"},{"authors":["Xinkai Zhao","Chaowei Fang","De-Jun Fan","Xutao Lin","Feng Gao","Guanbin Li"],"categories":null,"content":"","date":1642464000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1642464000,"objectID":"65169d0b42102cccf509eaf24e5c0570","permalink":"https://chaoweifang.github.io/publication/isbi2022crosslevel/","publishdate":"2022-01-18T00:00:00Z","relpermalink":"/publication/isbi2022crosslevel/","section":"publication","summary":"","tags":["Medical Image Analysis","Image Segmentation","Label-efficient Machine Learning","Semi-supervised Learning"],"title":"Cross-level Contrastive Learning and Consistency Constraint for Semi-supervised Medical Image Segmentation","type":"publication"},{"authors":["Chaowei Fang","Xue Li","Zhongyu Li","Licheng Jiao","Dingwen Zhang"],"categories":null,"content":"","date":1641859200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641859200,"objectID":"ac075b088ea02396b995746d28303bd2","permalink":"https://chaoweifang.github.io/publication/acta2022dualmodel/","publishdate":"2022-01-11T00:00:00Z","relpermalink":"/publication/acta2022dualmodel/","section":"publication","summary":"","tags":["Medical Image Analysis","Image Segmentation","Label-efficient Machine Learning","Semi-supervised Learning"],"title":"Interactive Dual-Model Learning for Semi-supervised Medical Image Segmentation (in Chinese)","type":"publication"},{"authors":["Baolian Qi","Gangming Zhao","Xin Wei","Chaowei Fang","Zhiqiang Chen","Jinpeng Li"],"categories":null,"content":"","date":1639267200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1639267200,"objectID":"c690b0ced836be2e6d3d3f967504148e","permalink":"https://chaoweifang.github.io/publication/bibm2021weakly/","publishdate":"2021-12-12T00:00:00Z","relpermalink":"/publication/bibm2021weakly/","section":"publication","summary":"","tags":["Medical Image Analysis","Disease Detection","Label-efficient Machine Learning","Weakly Supervised Learning"],"title":"Weakly Supervised Disease Localization in Chest X-rays via Looking into Image Relations","type":"publication"},{"authors":["Chaowei Fang","Haibin Tian","Dingwen Zhang","Qiang Zhang","Jungong Han","Junwei Han"],"categories":null,"content":"","date":1638489600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638489600,"objectID":"5a45957e137226149818c6ea6c30c2e2","permalink":"https://chaoweifang.github.io/publication/scis21tdf/","publishdate":"2021-12-03T00:00:00Z","relpermalink":"/publication/scis21tdf/","section":"publication","summary":"","tags":["Saliency Detection","Neural Network Design"],"title":"Densely Nested Top-Down Flows for Salient Object Detection","type":"publication"},{"authors":["Taiping Qu","Xiheng Wang","Chaowei Fang","Li Mao","Juan Li","Ping Li","Jinrong Qu","Xiuli Li","Huadan Xue","Yizhou Yu","Zhengyu Jin"],"categories":null,"content":"","date":1634083200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1634083200,"objectID":"873d7a36b84dc94febfd748c8863984c","permalink":"https://chaoweifang.github.io/publication/mia21m3net/","publishdate":"2021-10-13T00:00:00Z","relpermalink":"/publication/mia21m3net/","section":"publication","summary":"The complementation of arterial and venous phases visual information of CTs can help better distinguish the pancreas from its surrounding structures. However, the exploration of cross-phase contextual information is still under research in computeraided pancreas segmentation. This paper presents M 3 Net, a framework that integrates multi-scale multi-view information for multi-phase pancreas segmentation. The core of M 3 Net is built upon a dual-path network in which individual branches are set up for two phases. Cross-phase interactive connections bridging the two branches are introduced to interleave and integrate dual-phase complementary visual information. Besides, we further devise two types of non-local attention modules to enhance the high-level feature representation across phases. First, we design a location attention module to generate cross-phase reliable feature correlations to suppress the misalignment regions. Second, the depth-wise attention module is used to capture the channel dependencies and then strengthen feature representations. The experiment data consists of 224 internal CTs (106 normal and 118 abnormal) with 1mm slice thickness, and 66 external CTs (29 normal and 37 abnormal) with 5mm slice thickness. We achieve new state-of-the-art performance with average DSC of 91.19% on internal data, and promising result with average DSC of 86.34% on external data.","tags":["Medical Image Analysis","Image Segmentation","Neural Network Design"],"title":"M3Net: A Multi-scale Multi-view Framework for Multi-phase Pancreas Segmentation Based on Cross-phase Non-local Attention","type":"publication"},{"authors":["Junkai Huang","Chaowei Fang","Weikai Chen","Zhenhua Chai","Xiaolin Wei","Pengxu Wei","Liang Lin","Guanbin Li"],"categories":null,"content":"","date":1626998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1626998400,"objectID":"6fa1c881acf7cacfb9bcf065eb3cbf83","permalink":"https://chaoweifang.github.io/publication/iccv2021trash/","publishdate":"2021-07-23T00:00:00Z","relpermalink":"/publication/iccv2021trash/","section":"publication","summary":"Open-set semi-supervised learning~(open-set SSL) investigates a challenging but practical scenario where out-of-distribution (OOD) samples are contained in the unlabeled data. While the mainstream technique seeks to completely filter out the OOD samples for semi-supervised learning (SSL), we propose a novel training mechanism that could effectively exploit the presence of OOD data for enhanced feature learning while avoiding its adverse impact on the SSL. We achieve this goal by first introducing a warm-up training that leverages all the unlabeled data, including both the in-distribution (ID) and OOD samples. Specifically, we perform a pretext task that enforces our feature extractor to obtain a high-level semantic understanding of the training images, leading to more discriminative features that can benefit the downstream tasks. Since the OOD samples are inevitably detrimental to SSL, we propose a novel cross-modal matching strategy to detect OOD samples. Instead of directly applying binary classification~\\cite{yu2020multi}, we train the network to predict whether the data sample is matched to an assigned one-hot class label. The appeal of the proposed cross-modal matching over binary classification is the ability to generate a compatible feature space that aligns with the core classification task. Extensive experiments show that our approach substantially lifts the performance on open-set SSL and outperforms the state-of-the-art by a large margin.","tags":["Label-efficient Machine Learning","Semi-supervised Learning","Open-set Learning","Image Classification"],"title":"Trash to Treasure: Harvesting OOD Data with Cross-Modal Matching for Open-Set Semi-Supervised Learning","type":"publication"},{"authors":["Yiyang Huang","Xuefeng Liang","Chaowei Fang"],"categories":null,"content":"","date":1625788800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625788800,"objectID":"d8a20e1281bb6f5fbc8e5a5590ce97d4","permalink":"https://chaoweifang.github.io/publication/acmmm2021calip/","publishdate":"2021-07-09T00:00:00Z","relpermalink":"/publication/acmmm2021calip/","section":"publication","summary":"Lipreading, aiming at interpreting speech by watching the lip movements of the speaker, has great significance in human communication and speech understanding. Although has reached a feasible performance, lipreading still faces two crucial challenges: 1) the considerable lip movement variations cross different persons when they utter the same words; 2) the similar lip movements of people when they utter some confused phonemes. To tackle these two problems, we propose a novel lipreading framework, CALLip, which employs attribute learning and contrastive learning. The attribute learning extracts the speaker identity-aware features through a speaker recognition branch, which are able to normalize the lip shapes to eliminate cross-speaker variations. Considering that audio signals are intrinsically more distinguishable than visual signals, the contrastive learning is devised between visual and audio signals to enhance the discrimination of visual features and alleviant the viseme confusion problem. Experimental results show that CALLip does learn a better features of lip movements. The comparisons on both English and Chinese benchmark datasets, GRID and CMLR, demonstrate that CALLip outperforms six state-of-the-art lipreading methods without using any additional data.","tags":["Lip Reading","Representation Learning","Visual-audio Understanding"],"title":"CALLip: Lipreading using Contrastive and Attribute Learning","type":"publication"},{"authors":["Gangming Zhao","Chaowei Fang","Guanbin Li","Licheng Jiao","Yizhou Yu"],"categories":null,"content":"","date":1619654400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1619654400,"objectID":"b2228a5e54789717abbc85a430189383","permalink":"https://chaoweifang.github.io/publication/tmi21contra/","publishdate":"2021-04-29T00:00:00Z","relpermalink":"/publication/tmi21contra/","section":"publication","summary":"Identifying and locating diseases in chest Xrays are very challenging, due to the low visual contrast between normal and abnormal regions, and distortions caused by other overlapping tissues. An interesting phenomenon is that there exist many similar structures in the left and right parts of the chest, such as ribs, lung fields and bronchial tubes. This kind of similarities can be used to identify diseases in chest X-rays, according to the experience of broad-certificated radiologists. Aimed at improving the performance of existing detection methods, we propose a deep end-to-end module to exploit the contralateral context information for enhancing feature representations of disease proposals. First of all, under the guidance of the spine line, the spatial transformer network is employed to extract local contralateral patches, which can provide valuable context information for disease proposals. Then, we build up a specific module, based on both additive and subtractive operations, to fuse the features of the disease proposal and the contralateral patch. Our method can be integrated into both fully and weakly supervised disease detection frameworks. It achieves 33.17 AP50 on a carefully annotated private chest X-ray dataset which contains 31,000 images. Experiments on the NIH chest Xray dataset indicate that our method achieves state-of-theart performance in weakly-supervised disease localization.","tags":["Medical Image Analysis","Object Detection","Disease Diagnosis","Chest X-rays"],"title":"Contralaterally Enhanced Networks for Thoracic Disease Detection","type":"publication"},{"authors":["Xinkai Zhao","Chaowei Fang","Feng Gao","De-Jun FAN","Xutao Lin","Guanbin Li"],"categories":null,"content":"","date":1618272000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618272000,"objectID":"d6d1ef40856224ae4d43f2e57ed05cd3","permalink":"https://chaoweifang.github.io/publication/isbi2021intestine/","publishdate":"2021-04-13T00:00:00Z","relpermalink":"/publication/isbi2021intestine/","section":"publication","summary":"Capsule endoscopy is an evolutional technique for examining and diagnosing intractable gastrointestinal diseases. Because of the huge amount of data, analyzing capsule endoscope videos is very time-consuming and labor-intensive for gastrointestinal medicalists. The development of intelligent long video analysis algorithms for regional positioning and analysis of capsule endoscopic video is therefore essential to reduce the workload of clinicians and assist in improving the accuracy of disease diagnosis. In this paper, we propose a deep model to ground shooting range of small intestine from a capsule endoscope video which has duration of tens of hours. This is the first attempt to attack the small intestine grounding task using deep neural network method. We model the task as a 3-way classification problem, in which every video frame is categorized into esophagus/stomach, small intestine or colorectum. To explore long-range temporal dependency, a transformer module is built to fuse features of multiple neighboring frames. Based on the classification model, we devise an efficient search algorithm to efficiently locate the starting and ending shooting boundaries of the small intestine. Without searching the small intestine exhaustively in the full video, our method is implemented via iteratively separating the video segment along the direction to the target boundary in the middle. We collect 113 videos from a local hospital to validate our method. In the 5-fold cross validation, the average IoU between the small intestine segments located by our method and the ground-truths annotated by broad-certificated gastroenterologists reaches 0.945.","tags":["Medical Image Analysis","Neural Network Design","Capsule Endoscopy","Image/Video Classification"],"title":"Deep Transformers for Fast Small Intestine Grounding in Capsule Endoscope Video","type":"publication"},{"authors":["Dou Xu","Chang Cai","Chaowei Fang","Bin Kong","Jihua Zhu","Zhongyu Li"],"categories":null,"content":"","date":1618272000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618272000,"objectID":"f854ddbc4c9f107a1e2bc36ffac90dbd","permalink":"https://chaoweifang.github.io/publication/isbi2021da/","publishdate":"2021-04-13T00:00:00Z","relpermalink":"/publication/isbi2021da/","section":"publication","summary":"Annotating histopathological images is a time-consuming andlabor-intensive process, which requires broad-certificated pathologistscarefully examining large-scale whole-slide images from cells to tissues.Recent frontiers of transfer learning techniques have been widely investi-gated for image understanding tasks with limited annotations. However,when applied for the analytics of histology images, few of them can effec-tively avoid the performance degradation caused by the domain discrep-ancy between the source training dataset and the target dataset, suchas different tissues, staining appearances, and imaging devices. To thisend, we present a novel method for the unsupervised domain adaptationin histopathological image analysis, based on a backbone for embeddinginput images into a feature space, and a graph neural layer for propa-gating the supervision signals of images with labels. The graph model isset up by connecting every image with its close neighbors in the embed-ded feature space. Then graph neural network is employed to synthesizenew feature representation from every image. During the training stage,target samples with confident inferences are dynamically allocated withpseudo labels. The cross-entropy loss function is used to constrain thepredictions of source samples with manually marked labels and targetsamples with pseudo labels. Furthermore, the maximum mean diversityis adopted to facilitate the extraction of domain-invariant feature repre-sentations, and contrastive learning is exploited to enhance the categorydiscrimination of learned features. In experiments of the unsupervised do-main adaptation for histopathological image classification, our methodachieves state-of-the-art performance on four public datasets.","tags":["Medical Image Analysis","Histopathological Image","Label-efficient Machine Learning","Domain Adaptation"],"title":"Graph Neural Networks for Unsupervised Domain Adaptation of Histopathological ImageAnalytics","type":"publication"},{"authors":["Feida Zhu","Chaowei Fang","Kai-Kuang Ma"],"categories":null,"content":"","date":1598054400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598054400,"objectID":"b65cfc59e7123fc49c6d7a70b2856015","permalink":"https://chaoweifang.github.io/publication/tip20pnen/","publishdate":"2020-08-22T00:00:00Z","relpermalink":"/publication/tip20pnen/","section":"publication","summary":"Existing neural networks proposed for low-level image processing tasks are usually implemented by stacking convolution layers with limited kernel size. Every convolution layer merely involves in context information from a small local neighborhood. More contextual features can be explored as more convolution layers are adopted. However it is difficult and costly to take full advantage of long-range dependencies. We propose a novel non-local module, Pyramid Non-local Block, to build up connection between every pixel and all remain pixels. The proposed module is capable of efficiently exploiting pairwise dependencies between different scales of low-level structures. The target is fulfilled through first learning a query feature map with full resolution and a pyramid of reference feature maps with downscaled resolutions. Then correlations with multi-scale reference features are exploited for enhancing pixel-level feature representation. The calculation procedure is economical considering memory consumption and computational cost. Based on the proposed module, we devise a Pyramid Non-local Enhanced Networks for edge-preserving image smoothing which achieves state-of-the-art performance in imitating three classical image smoothing algorithms. Additionally, the pyramid non-local block can be directly incorporated into convolution neural networks for other image restoration tasks. We integrate it into two existing methods for image denoising and single image super-resolution, achieving consistently improved performance.","tags":["Low-level Image Processing","Neural Network Design","Image/Video Super-resolution","Image Denoising","Image Filtering"],"title":"PNEN: Pyramid Non-Local Enhanced Networks","type":"publication"},{"authors":["Jixin Wang","Sanping Zhou","Chaowei Fang","Le Wang","Jinjun Wang"],"categories":null,"content":"","date":1592870400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592870400,"objectID":"04a4046541055c984b0e6cb1ce7abded","permalink":"https://chaoweifang.github.io/publication/miccai20metamask/","publishdate":"2020-06-23T00:00:00Z","relpermalink":"/publication/miccai20metamask/","section":"publication","summary":"Deep neural networks have achieved satisfactory performance in piles of medical image analysis tasks. However the training of deep neural network requires a large amount of samples with high-quality annotations. In medical image segmentation, it is very laborious and expensive to acquire precise pixel-level annotations. Aiming at training deep segmentation models on datasets with probably corrupted annotations, we propose a novel Meta Corrupted Pixels Mining (MCPM) method based on a simple meta mask network. Our method is targeted at automatically estimate a weighting map to evaluate the importance of every pixel in the learning of segmentation network. The meta mask network which regards the loss value map of the predicted segmentation results as input, is capable of identifying out corrupted layers and allocating small weights to them. An alternative algorithm is adopted to train the segmentation network and the meta mask network, simultaneously. Extensive experimental results on LIDC-IDRI and LiTS datasets show that our method outperforms state-of-the-art approaches which are devised for coping with corrupted annotations.","tags":["Medical Image Analysis","Image Segmentation","Label-efficient Machine Learning","Learning with Noisy Labels"],"title":"Meta Corrupted Pixels Mining for Medical Image Segmentation","type":"publication"},{"authors":["Zhongyu Li","Xiayue Fan","Zengyi Shang","Lina Zhang","Haotian Zhen","Chaowei Fang"],"categories":null,"content":"","date":1583971200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583971200,"objectID":"0414ff5b89c614032882d3518c0fcabc","permalink":"https://chaoweifang.github.io/publication/neurocomp20adv/","publishdate":"2020-03-12T00:00:00Z","relpermalink":"/publication/neurocomp20adv/","section":"publication","summary":"Benefited from advances of neuron tracing techniques, the ever-increasing number of digitally reconstructed 3D neuron images have greatly facilitated the research in neuromorphology. However, the sheer volume and the complexity of these 3D neuron data pose significant challenges for computational analytics, e.g., effectively finding neurons sharing similar morphologies, identifying neuron types, correlating neuron morphologies with properties, all of which require accurate measuring and fast indexing methods especially designed for the massive 3D neuronal images. In this paper, we present an accurate and efficient framework for the computational analytics of 3D neuronal structures based on advances of deep learning and data mining techniques. Particularly, unlike previous methods quantitatively describe neurons by measuring pre-defined metrics according to the tree-topological structures, we first develop a new method for the morphological feature representation by a proposed 3D neuron mapping and a modified generative adversarial networks (GANs). Subsequently, considering the computational complexity when retrieving large-scale neuron datasets, we integrate the neuron features with graph-based indexing, which can significantly improve the retrieval efficiency without losing accuracy. Experimental results show that our framework can effectively measure the similarity among massive neurons (e.g.,  neurons), outperforming state-of-the-arts with more than 10% in accuracy and hundreds of times in efficiency improvements.","tags":["Representation Learning","3D Neural Morphology"],"title":"Towards Computational Analytics of 3D Neuron Images using Deep Adversarial Learning","type":"publication"},{"authors":["Chaowei Fang","Guanbin Li","Xiaoguang Han","Yizhou Yu"],"categories":null,"content":"","date":1573603200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1573603200,"objectID":"290a43832b803ae99afe461c45ba7073","permalink":"https://chaoweifang.github.io/publication/tip19secn/","publishdate":"2019-11-13T00:00:00Z","relpermalink":"/publication/tip19secn/","section":"publication","summary":"As a domain-specific super-resolution problem, facial image hallucination has enjoyed a series of breakthroughs thanks to the advances of deep convolutional neural networks. However, the direct migration of existing methods to video is still difficult to achieve good performance due to its lack of alignment and consistency modelling in temporal domain. Taking advantage of high inter-frame dependency in videos, we propose a self-enhanced convolutional network for facial video hallucination. It is implemented by making full usage of preceding super-resolved frames and a temporal window of adjacent low-resolution frames. Specifically, the algorithm first obtains the initial high-resolution inference of each frame by taking into consideration a sequence of consecutive low-resolution inputs through temporal consistency modelling. It further recurrently exploits the reconstructed results and intermediate features of a sequence of preceding frames to improve the initial super-resolution of the current frame by modelling the coherence of structural facial features across frames. Quantitative and qualitative evaluations demonstrate the superiority of the proposed algorithm against state-of-the-art methods. Moreover, our algorithm also achieves excellent performance in the task of general video super-resolution in a single-shot setting.","tags":["Low-level Image Processing","Neural Network Design","Image/Video Super-resolution"],"title":"Self-Enhanced Convolutional Network for Facial Video Hallucination","type":"publication"},{"authors":["Chaowei Fang","Guanbin Li","Chengwei Pan","Yiming Li","Yizhou Yu"],"categories":null,"content":"","date":1571270400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571270400,"objectID":"83941dd75a5575d091c6b9dde4020e43","permalink":"https://chaoweifang.github.io/publication/miccai19ggpfn/","publishdate":"2019-10-17T00:00:00Z","relpermalink":"/publication/miccai19ggpfn/","section":"publication","summary":"Recently 3D volumetric organ segmentation attracts much research interest in medical image analysis due to its significance in com- puter aided diagnosis. This paper aims to address the pancreas segmen- tation task in 3D computed tomography volumes. We propose a novel end-to-end network, Globally Guided Progressive Fusion Network, as an effective and efficient solution to volumetric segmentation, which involves both global features and complicated 3D geometric information. A pro- gressive fusion network is devised to extract 3D information from a mod- erate number of neighboring slices and predict a probability map for the segmentation of each slice. An independent branch for excavating global features from downsampled slices is further integrated into the network. Extensive experimental results demonstrate that our method achieves state-of-the-art performance on two pancreas datasets.","tags":["Medical Image Analysis","Image Segmentation","Neural Network Design"],"title":"Globally Guided Progressive Fusion Network for 3D Pancreas Segmentation","type":"publication"},{"authors":["Chaowei Fang","Zicheng Liao","Yizhou Yu"],"categories":null,"content":"","date":1559347200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559347200,"objectID":"b307c8157752d44c428fd7e4f92f6504","permalink":"https://chaoweifang.github.io/publication/pami19pfe/","publishdate":"2019-06-01T00:00:00Z","relpermalink":"/publication/pami19pfe/","section":"publication","summary":"We introduce a new multi-dimensional nonlinear embedding, Piecewise Flat Embedding (PFE), for image segmentation. Based on the theory of sparse signal recovery, piecewise flat embedding with diverse channels attempts to recover a piecewise constant image representation with sparse region boundaries and sparse cluster value scattering. The resultant piecewise flat embedding exhibits interesting properties such as suppressing slowly varying signals, and offers an image representation with higher region identifiability which is desirable for image segmentation or high-level semantic analysis tasks. We formulate our embedding as a variant of the Laplacian Eigenmap embedding with an `$L_{1,p}(0 \\leq 1)$` regularization term to promote sparse solutions. First, we devise a two-stage numerical algorithm based on Bregman iterations to compute `$ L_{1,1} $`-regularized piecewise flat embeddings. We further generalize this algorithm through iterative reweighting to solve the general `$L_{1,p}$`-regularized problem. To demonstrate its efficacy, we integrate PFE into two existing image segmentation frameworks, segmentation based on clustering and hierarchical segmentation based on contour detection. Experiments on four major benchmark datasets, BSDS500, MSRC, Stanford Background Dataset, and PASCAL Context, show that segmentation algorithms incorporating our embedding achieve significantly improved results.","tags":["Representation Learning","Image Segmentation"],"title":"Piecewise Flat Embedding for Image Segmentation","type":"publication"},{"authors":["Zhongyu Li","Chaowei Fang","Shaoting Zhang"],"categories":null,"content":"","date":1523059200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1523059200,"objectID":"422d1451cd2de2203f6a3a5edb9be768","permalink":"https://chaoweifang.github.io/publication/isbi2018neural/","publishdate":"2018-04-07T00:00:00Z","relpermalink":"/publication/isbi2018neural/","section":"publication","summary":"With the ever-increasing number of digitally reconstructed neurons, computational analytics of 3D neuronal morphology has become a new avenue to understand neuroanatomical structures and functional properties. However, traditional methods cannot well identify and represent neuronal morphology, especially when tackling large-scale and diverse neuron datasets. In this paper, we propose a deep learning based framework for the representation of 3D neuron morphology. At first, considering the neuronal tree-structures are usually very sparse in 3D space, we project each 3D neuron into 2D images with three angles of view. The projective strategy can preserve the spatial morphologies from the original data. Subsequently, as there are no sufficient annotations for each neuron, we introduce an unsupervised deep neural network to automatically learn neuron features from the projected 2D images. The deep features are then combined with traditional features for accurate neuron representation. Experimental results validate the effectiveness of our method by searching similar samples on a public database including 58; 000 neurons. Moreover, we demonstrate that the traditional hand-crafted features are complementary with deep features in the representation of 3D neuron morphology.","tags":["Representation Learning","3D Neuron Morphology"],"title":"Deep Feature Representation for The Computational Analytics of 3D Neuronal Morphology","type":"publication"},{"authors":["Wei Zhang","Chaowei Fang","Guanbin Li"],"categories":null,"content":"","date":1494547200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1494547200,"objectID":"ea9c039e470caa6533223db2c43eb19a","permalink":"https://chaoweifang.github.io/publication/jcst17color/","publishdate":"2017-05-12T00:00:00Z","relpermalink":"/publication/jcst17color/","section":"publication","summary":"Grayscale image colorization is an important computer graphics problem with a variety of applications. Recent fully automatic colorization methods have made impressive progress by formulating image colorization as a pixel-wise prediction task and utilizing deep convolutional neural networks. Though tremendous improvements have been made, the result of automatic colorization is still far from perfect. Specifically, there still exist common pitfalls in maintaining color consistency in homogeneous regions as well as precisely distinguishing colors near region boundaries. To tackle these problems, we propose a novel fully automatic colorization pipeline which involves a boundary-guided CRF (conditional random field) and a CNN-based color transform as post-processing steps. In addition, as there usually exist multiple plausible colorization proposals for a single image, automatic evaluation for different colorization methods remains a challenging task. We further introduce two novel automatic evaluation schemes to efficiently assess colorization quality in terms of spatial coherence and localization. Comprehensive experiments demonstrate great quality improvement in results of our proposed colorization method under multiple evaluation metrics.","tags":["Low-level Image Processing","Image Colorization"],"title":"Automatic Colorization with Improved Spatial Coherence and Boundary Localization","type":"publication"},{"authors":["Yizhou Yu","Chaowei Fang","Zicheng Liao"],"categories":null,"content":"","date":1450224000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1450224000,"objectID":"f66b086051da04d11c5287e7be46b216","permalink":"https://chaoweifang.github.io/publication/iccv2015pfe/","publishdate":"2015-12-16T00:00:00Z","relpermalink":"/publication/iccv2015pfe/","section":"publication","summary":"Image segmentation is a critical step in many computer vision tasks, including high-level visual recognition and scene understanding as well as low-level photo and video processing. In this paper, we propose a new nonlinear embedding, called piecewise flat embedding, for image segmentation. Based on the theory of sparse signal recovery, piecewise flat embedding attempts to identify segment boundaries while significantly suppressing variations within segments. We adopt an `$L_1$`-regularized energy term in the formulation to promote sparse solutions. We further devise an effective two-stage numerical algorithm based on Bregman iterations to solve the proposed embedding. Piecewise flat embedding can be easily integrated into existing image segmentation frameworks, including segmentation based on spectral clustering and hierarchical segmentation based on contour detection. Experiments on BSDS500 indicate that segmentation algorithms incorporating this embedding can achieve significantly improved results in both frameworks. ","tags":["Representation Learning","Image Segmentation"],"title":"Piecewise Flat Embedding for Image Segmentation","type":"publication"},{"authors":["Zhongyu Li","Jihua Zhu","Ke Lan","Chen Li","Chaowei Fang"],"categories":null,"content":"","date":1418256000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1418256000,"objectID":"431e7b11ff83291dfba9d7b103a3361b","permalink":"https://chaoweifang.github.io/publication/3dv2015register/","publishdate":"2014-12-11T00:00:00Z","relpermalink":"/publication/3dv2015register/","section":"publication","summary":"","tags":["Point Cloud Registration"],"title":"Improved Techniques for Multi-view Registration with Motion Averaging","type":"publication"}]